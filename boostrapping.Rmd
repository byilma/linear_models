---
title: "boostrapping"
output: github_document
---

```{r setup, include = F}
library(tidyverse)
library(modelr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
```



## Simulate data

```{r}
n_samp = 250

sim_df_const = 
  tibble(
    x = rnorm(n_samp, 1, 1),
    error = rnorm(n_samp, 0, 1),
    y = 2 + 3 * x + error
  )

sim_df_nonconst = sim_df_const %>% 
  mutate(
  error = error * .75 * x,
  y = 2 + 3 * x + error
)

```


Plot the datasets

```{r}
sim_df_const %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  labs(
    title = "A nice distribution that meets the theoretical assumptions of a linear regression", 
    subtitle = "Linearity & homosecedasticity are both met | independence & normality are assumed"
  )



sim_df_nonconst %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  labs(
    title = "Not a nice distribution | homoscedasticity assumption not met", 
    subtitle = "Because this key assumption is not met, we can't make inferences with confidence"
  )
```


Check the parameter estimates (standard error, estimates etc ... )
The lm function gives us these estimates assuming that our statistical assumptions have been met on our data

```{r}
lm(y ~ x, data = sim_df_const) %>% broom::tidy()
lm(y ~ x, data = sim_df_nonconst) %>% broom::tidy()
```

Even though the error terms for `sim_df_nonconst` vary widely compared to the error terms of the `sim_df_const`, the std.error that our `lm` gave us was similar for both models. THIS is the problem we're trying to solve with `Boostrapping` ... when we can't make theoretical assumptions (our keys assumptions have been violated) we rely on `bootsrapping` to get a sense for the distribution our data follows using emprical evidence. 

`boostrap` time

## Draw one bootstrap sample

Writing a function that takes a bootstrap sample from a given dataset
```{r}
boot_sample = function(df) {
  
  sample_frac(df, size = 1, replace = TRUE) %>% 
    arrange(x)
  
  #size = 1 specifies that we want to draw a sample the same size as our dataframe & replace = TRUE tells is that we want to draw samples with replacements, such that multiple obs may be resampled. 
  # Why do we want it to be the same size? because our variance estimates (& vis a vi our confidence intervals) are dependent on sample size. That's why we "size = 1" 
  
}

```


Check if this works ... Plotty plot
```{r}
boot_sample(sim_df_nonconst) %>%  
  ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .3) + 
  geom_smooth(method = "lm") + 
  ylim(-5, 16)
  
```

When you rerun the bootstrapping you get different estimates/slopes because bootrstapping (re)samples randomly
```{r}
boot_sample(sim_df_nonconst) %>%  
   lm(y ~ x, data = .) %>% 
  broom::tidy()
```


## Many samples and analysis

```{r}
boot_straps = 
  tibble(
    strap_number = 1:1000,
    strap_sample = rerun(1000, boot_sample(sim_df_nonconst))
  )
```



Can I run my analysis on these ... ? yes! using stuff we've used before

```{r}

boot_results = 
  boot_straps %>% 
  mutate(
    models = map(.x = strap_sample, ~lm(y ~ x, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  select(strap_number, results) %>% 
  unnest(results)
```


What do I have now? 

We now want to know the distribution of the estimated intercept and estimated coefficient (slope)


Compare the error of the estimate and the slope you got from the bootstrap samples to the error terms you got from the linear models:

Notice how the std.errors you get from the bootsrap model better approximate the distribution of the error terms we see in `sim_df_nonconst` (check scatter plot) with a lower std.error around the intercept & a larger std.error for the slope oi
```{r}
boot_results %>% 
  group_by(term) %>% 
  summarize(
    mean_est  = mean(estimate), #1st moment
    sd_est = sd(estimate) #3rd (?) moment
  )

lm(y ~ x, data = sim_df_nonconst) %>% broom::tidy()
```


What we can then do is go ahead and construct the confidence intervals using the bootstrap results

First, let's look at the distributions of the estimate

```{r}
boot_results %>% 
  filter(term == "x") %>% 
  ggplot(aes(x = estimate)) + 
  geom_density()
```

Under bootstrapping, i.e., under repeated sampling, without relying on any theoretical assumptions, the distributions of the estimated slope (`x`) looks like this ^^ .... so now, if I want to construct confidence intervals, I can go to the tail ends, and "chop-off" or set cut-off points at 2.5% of the distribution, as it were. 

construct boostrap CI 

```{r}
boot_results %>% 
  group_by(term) %>% 
  summarize(
    ci_lower = quantile(estimate, 0.025), # quantile() gives me the quantiles of the estimates at 2.5% 
    ci_upper = quantile(estimate, 0.975)  # quantile() gives me the quantiles of the estimates at 97.5% 
  )

```


## Bootstrap using modelr


Can we simplify anything from the above 

The function `bootstrap` in `modelr` carries out lets us bootstrap from a given dataset ... which is essentially what we did with our function `boot_sample`

Note how the `bootstrap` function gives us a "tibble" object instead of a `tibble`
```{r}
sim_df_const %>% 
  bootstrap(1000, id = "strap_number") %>% 
  mutate(
    models = map(.x = strap, ~lm(y ~ x, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  select(strap_number, results) %>% 
  unnest(results)  %>%  
  group_by(term) %>% 
  summarize(
    mean_est  = mean(estimate), #1st moment
    sd_est = sd(estimate) #3rd (?) moment
  )
```








